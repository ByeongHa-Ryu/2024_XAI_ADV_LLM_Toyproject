{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy==1.23.1 in /home/work/.local/lib/python3.10/site-packages (1.23.1)\n",
            "1.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.1\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8qG3eATL1hjX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqbezJpASoXf",
        "outputId": "8a660e81-ccb9-479e-8af6-135aa4213896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "XaJ2RmAM8PvI",
        "outputId": "0bf01324-d046-452c-9f09-51f5f01dd2b2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e630500f-df31-472a-a019-edb1ade7277c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>상호명</th>\n",
              "      <th>상권업종대분류명</th>\n",
              "      <th>상권업종중분류명</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>엠마스키친</td>\n",
              "      <td>음식</td>\n",
              "      <td>서양식</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>팽성농산물센터</td>\n",
              "      <td>소매</td>\n",
              "      <td>식료품 소매</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>역전할머니맥주평택궁리점</td>\n",
              "      <td>음식</td>\n",
              "      <td>주점</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>혜윰건축사사무소</td>\n",
              "      <td>과학·기술</td>\n",
              "      <td>기술 서비스</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>양촌리민물장어</td>\n",
              "      <td>음식</td>\n",
              "      <td>한식</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796862</th>\n",
              "      <td>다리미</td>\n",
              "      <td>소매</td>\n",
              "      <td>섬유·의복·신발 소매</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796863</th>\n",
              "      <td>청하중화요리</td>\n",
              "      <td>음식</td>\n",
              "      <td>중식</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796864</th>\n",
              "      <td>대칭점</td>\n",
              "      <td>음식</td>\n",
              "      <td>비알코올</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796865</th>\n",
              "      <td>나베르떼헤어</td>\n",
              "      <td>수리·개인</td>\n",
              "      <td>이용·미용</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796866</th>\n",
              "      <td>대단한탕후루인천왕길점</td>\n",
              "      <td>음식</td>\n",
              "      <td>기타 간이</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>796867 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e630500f-df31-472a-a019-edb1ade7277c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e630500f-df31-472a-a019-edb1ade7277c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e630500f-df31-472a-a019-edb1ade7277c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 상호명 상권업종대분류명     상권업종중분류명\n",
              "0              엠마스키친       음식          서양식\n",
              "1            팽성농산물센터       소매       식료품 소매\n",
              "2       역전할머니맥주평택궁리점       음식           주점\n",
              "3           혜윰건축사사무소    과학·기술       기술 서비스\n",
              "4            양촌리민물장어       음식           한식\n",
              "...              ...      ...          ...\n",
              "796862           다리미       소매  섬유·의복·신발 소매\n",
              "796863        청하중화요리       음식           중식\n",
              "796864           대칭점       음식        비알코올 \n",
              "796865        나베르떼헤어    수리·개인        이용·미용\n",
              "796866   대단한탕후루인천왕길점       음식        기타 간이\n",
              "\n",
              "[796867 rows x 3 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_pickle('dataset_small.pkl')\n",
        "data # 796867"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "QugfvCTI1baI",
        "outputId": "19090b08-e243-4201-9a2a-4dc951d0e166"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-84c6b2f5-f2ad-4fc7-b747-a42f4211958e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>상호명</th>\n",
              "      <th>상권업종대분류명</th>\n",
              "      <th>상권업종중분류명</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>엠마스키친</td>\n",
              "      <td>9</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>팽성농산물센터</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>역전할머니맥주평택궁리점</td>\n",
              "      <td>9</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>혜윰건축사사무소</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>양촌리민물장어</td>\n",
              "      <td>9</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84c6b2f5-f2ad-4fc7-b747-a42f4211958e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84c6b2f5-f2ad-4fc7-b747-a42f4211958e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84c6b2f5-f2ad-4fc7-b747-a42f4211958e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            상호명  상권업종대분류명  상권업종중분류명\n",
              "0         엠마스키친         9        33\n",
              "1       팽성농산물센터         4        41\n",
              "2  역전할머니맥주평택궁리점         9        66\n",
              "3      혜윰건축사사무소         0         8\n",
              "4       양촌리민물장어         9        73"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder = LabelEncoder()\n",
        "label_dicts = {} \n",
        "\n",
        "for column in ['상권업종대분류명',\t'상권업종중분류명']:\n",
        "  encoder.fit(data[column])\n",
        "  label_dict = dict(zip(encoder.transform(encoder.classes_), encoder.classes_))\n",
        "  data[column] = encoder.transform(data[column])\n",
        "  label_dicts[column] = label_dict  # 해당 컬럼의 딕셔너리를 저장\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OaxsX0RaG-6Y"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
        "                 pad, pair):\n",
        "\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V1a8yzBq_7xK"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier1(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 10, # big\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier1, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BLNMl6bpCtIH"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 75, # mid\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier2, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tGfRCmWgDEyJ"
      },
      "outputs": [],
      "source": [
        "def prepare_data(text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        return_token_type_ids=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "    token_type_ids = encoding['token_type_ids']\n",
        "    valid_length = torch.tensor([torch.sum(attention_mask[0])], dtype=torch.long)\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, valid_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, input_ids, attention_mask, token_type_ids, valid_length):\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    token_type_ids = token_type_ids.to(device)\n",
        "    valid_length = valid_length.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, valid_length, token_type_ids)\n",
        "        probabilities = torch.softmax(outputs, dim=1) \n",
        "        predicted_class = torch.argmax(probabilities, dim=1)\n",
        "        return predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = data[['상호명', '상권업종대분류명']] # big\n",
        "df2 = data[['상호명', '상권업종중분류명']] # mid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "train1, test1 = train_test_split(df1, test_size=0.25, shuffle = True, random_state=0)\n",
        "train2, test2 = train_test_split(df2, test_size=0.25, shuffle = True, random_state=0)\n",
        "\n",
        "train1 = [[i, str(j)] for i, j in zip(train1['상호명'], train1['상권업종대분류명'])]\n",
        "train2 = [[i, str(j)] for i, j in zip(train2['상호명'], train2['상권업종중분류명'])]\n",
        "\n",
        "test1 = [[i, str(j)] for i, j in zip(test1['상호명'], test1['상권업종대분류명'])]\n",
        "test2 = [[i, str(j)] for i, j in zip(test2['상호명'], test2['상권업종중분류명'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
        "tok = tokenizer.tokenize\n",
        "\n",
        "# parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mwooXI_JHd_e"
      },
      "outputs": [],
      "source": [
        "# big\n",
        "train_set_data1 = BERTDataset(train1, 0, 1, tok, vocab, max_len, True, False)\n",
        "test_set_data1 = BERTDataset(test1, 0, 1, tok, vocab, max_len, True, False)\n",
        "train_dataloader1 = torch.utils.data.DataLoader(train_set_data1, batch_size=batch_size, num_workers=2)\n",
        "test_dataloader1 = torch.utils.data.DataLoader(test_set_data1, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "# mid\n",
        "train_set_data2 = BERTDataset(train2, 0, 1, tok, vocab, max_len, True, False)\n",
        "test_set_data2 = BERTDataset(test2, 0, 1, tok, vocab, max_len, True, False)\n",
        "train_dataloader2 = torch.utils.data.DataLoader(train_set_data2, batch_size=batch_size, num_workers=2)\n",
        "test_dataloader2 = torch.utils.data.DataLoader(test_set_data2, batch_size=batch_size, num_workers=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Vowz_LR-EFSM",
        "outputId": "a9c1c340-36ce-4c75-f0d0-56b09c542475"
      },
      "outputs": [],
      "source": [
        "# train - big\n",
        "model1 = BERTClassifier1(bertmodel, dr_rate=0.5).to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model1.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model1.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "t_total = len(train_dataloader1) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    model1.train()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader1)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model1(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model1.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader1)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model1(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model1, 'model/model1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-22-0d7298afdfa5>:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader2)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc51c5e502484c7da743dc021ca8bcd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1 loss 4.3969502449035645 train acc 0.015625\n",
            "epoch 1 batch id 201 loss 4.121189117431641 train acc 0.036069651741293535\n",
            "epoch 1 batch id 401 loss 3.9892735481262207 train acc 0.06729270573566085\n",
            "epoch 1 batch id 601 loss 3.8084254264831543 train acc 0.08353265391014975\n",
            "epoch 1 batch id 801 loss 3.5047385692596436 train acc 0.11060393258426966\n",
            "epoch 1 batch id 1001 loss 2.8618643283843994 train acc 0.14434003496503497\n",
            "epoch 1 batch id 1201 loss 2.813129186630249 train acc 0.18230901332223148\n",
            "epoch 1 batch id 1401 loss 2.6014840602874756 train acc 0.22003256602426838\n",
            "epoch 1 batch id 1601 loss 2.2721054553985596 train acc 0.25246915990006247\n",
            "epoch 1 batch id 1801 loss 2.6570253372192383 train acc 0.28064269850083284\n",
            "epoch 1 batch id 2001 loss 2.1423017978668213 train acc 0.30576899050474765\n",
            "epoch 1 batch id 2201 loss 1.9793694019317627 train acc 0.32788363243980007\n",
            "epoch 1 batch id 2401 loss 1.7088935375213623 train acc 0.3480125468554769\n",
            "epoch 1 batch id 2601 loss 2.215780735015869 train acc 0.3656225970780469\n",
            "epoch 1 batch id 2801 loss 1.7379287481307983 train acc 0.38087401820778294\n",
            "epoch 1 batch id 3001 loss 1.6870306730270386 train acc 0.3947330056647784\n",
            "epoch 1 batch id 3201 loss 1.7681742906570435 train acc 0.4075874726647922\n",
            "epoch 1 batch id 3401 loss 1.8050259351730347 train acc 0.4191414289914731\n",
            "epoch 1 batch id 3601 loss 1.8333585262298584 train acc 0.4296506178839211\n",
            "epoch 1 batch id 3801 loss 1.6994973421096802 train acc 0.4389922059984215\n",
            "epoch 1 batch id 4001 loss 1.5016125440597534 train acc 0.44720069982504373\n",
            "epoch 1 batch id 4201 loss 2.125993251800537 train acc 0.45526362770768863\n",
            "epoch 1 batch id 4401 loss 2.048905849456787 train acc 0.4624872188139059\n",
            "epoch 1 batch id 4601 loss 1.7871671915054321 train acc 0.4693341121495327\n",
            "epoch 1 batch id 4801 loss 1.8457978963851929 train acc 0.47558776817329723\n",
            "epoch 1 batch id 5001 loss 1.753013014793396 train acc 0.4814224655068986\n",
            "epoch 1 batch id 5201 loss 1.2943530082702637 train acc 0.4872710776773697\n",
            "epoch 1 batch id 5401 loss 1.5989875793457031 train acc 0.49243195704499165\n",
            "epoch 1 batch id 5601 loss 1.6574991941452026 train acc 0.49727727191572935\n",
            "epoch 1 batch id 5801 loss 1.8630661964416504 train acc 0.5018019522496121\n",
            "epoch 1 batch id 6001 loss 1.3323801755905151 train acc 0.506399975004166\n",
            "epoch 1 batch id 6201 loss 1.4055712223052979 train acc 0.5106409248508305\n",
            "epoch 1 batch id 6401 loss 1.311401605606079 train acc 0.5147169387595688\n",
            "epoch 1 batch id 6601 loss 1.2026047706604004 train acc 0.5181459248598698\n",
            "epoch 1 batch id 6801 loss 1.5875122547149658 train acc 0.5216535252168799\n",
            "epoch 1 batch id 7001 loss 1.2277326583862305 train acc 0.5251348021711184\n",
            "epoch 1 batch id 7201 loss 1.5400519371032715 train acc 0.5281905290931815\n",
            "epoch 1 batch id 7401 loss 1.5326253175735474 train acc 0.5312816680178354\n",
            "epoch 1 batch id 7601 loss 1.1826122999191284 train acc 0.5342553611366926\n",
            "epoch 1 batch id 7801 loss 1.2245374917984009 train acc 0.5368582553518779\n",
            "epoch 1 batch id 8001 loss 1.2684009075164795 train acc 0.5396024715660542\n",
            "epoch 1 batch id 8201 loss 2.0211398601531982 train acc 0.5422166504084868\n",
            "epoch 1 batch id 8401 loss 1.4590187072753906 train acc 0.5445315289846446\n",
            "epoch 1 batch id 8601 loss 1.5531871318817139 train acc 0.5469331327752587\n",
            "epoch 1 batch id 8801 loss 1.579836368560791 train acc 0.5491812009998864\n",
            "epoch 1 batch id 9001 loss 2.1344926357269287 train acc 0.5513640845461616\n",
            "epoch 1 batch id 9201 loss 1.4143340587615967 train acc 0.5532958374089773\n",
            "epoch 1 train acc 0.5546810864832067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-0d7298afdfa5>:41: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader2)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e29ffee508c140b981a3fbb1adfc31f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3113 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 test acc 0.6556049720723497\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "540ef94dad2e4436893b83b48e252951",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1 loss 1.432727336883545 train acc 0.625\n",
            "epoch 2 batch id 201 loss 1.3974932432174683 train acc 0.6442786069651741\n",
            "epoch 2 batch id 401 loss 1.4551588296890259 train acc 0.6435473815461347\n",
            "epoch 2 batch id 601 loss 1.4221612215042114 train acc 0.6468646006655574\n",
            "epoch 2 batch id 801 loss 1.5431731939315796 train acc 0.6481351435705368\n",
            "epoch 2 batch id 1001 loss 1.6366784572601318 train acc 0.6479458041958042\n",
            "epoch 2 batch id 1201 loss 1.3046950101852417 train acc 0.6483659450457951\n",
            "epoch 2 batch id 1401 loss 1.3403953313827515 train acc 0.649246074232691\n",
            "epoch 2 batch id 1601 loss 1.5414485931396484 train acc 0.649554965646471\n",
            "epoch 2 batch id 1801 loss 1.6425518989562988 train acc 0.6497431982232094\n",
            "epoch 2 batch id 2001 loss 1.098631739616394 train acc 0.6501749125437282\n",
            "epoch 2 batch id 2201 loss 1.151824951171875 train acc 0.6505281690140845\n",
            "epoch 2 batch id 2401 loss 1.1078087091445923 train acc 0.651173990004165\n",
            "epoch 2 batch id 2601 loss 1.2754985094070435 train acc 0.6520028354479046\n",
            "epoch 2 batch id 2801 loss 1.451088309288025 train acc 0.6523060960371296\n",
            "epoch 2 batch id 3001 loss 1.329037070274353 train acc 0.6527303398867045\n",
            "epoch 2 batch id 3201 loss 1.1674000024795532 train acc 0.653428616057482\n",
            "epoch 2 batch id 3401 loss 1.5209637880325317 train acc 0.6539528815054396\n",
            "epoch 2 batch id 3601 loss 1.457619547843933 train acc 0.6545056928630936\n",
            "epoch 2 batch id 3801 loss 1.4137712717056274 train acc 0.654909892133649\n",
            "epoch 2 batch id 4001 loss 1.1148080825805664 train acc 0.6549104911272182\n",
            "epoch 2 batch id 4201 loss 1.6981877088546753 train acc 0.6553461973339682\n",
            "epoch 2 batch id 4401 loss 1.410871148109436 train acc 0.655749403544649\n",
            "epoch 2 batch id 4601 loss 1.5208216905593872 train acc 0.6561617039773963\n",
            "epoch 2 batch id 4801 loss 1.539311408996582 train acc 0.6564517808789836\n",
            "epoch 2 batch id 5001 loss 1.5602105855941772 train acc 0.6570873325334933\n",
            "epoch 2 batch id 5201 loss 1.1361192464828491 train acc 0.6577461065179773\n",
            "epoch 2 batch id 5401 loss 1.52957022190094 train acc 0.6581796195149047\n",
            "epoch 2 batch id 5601 loss 1.5530468225479126 train acc 0.658476164970541\n",
            "epoch 2 batch id 5801 loss 1.6211974620819092 train acc 0.6587630365454232\n",
            "epoch 2 batch id 6001 loss 1.1539157629013062 train acc 0.6593067822029661\n",
            "epoch 2 batch id 6201 loss 1.2722043991088867 train acc 0.6598003346234478\n",
            "epoch 2 batch id 6401 loss 1.1749216318130493 train acc 0.660324070457741\n",
            "epoch 2 batch id 6601 loss 1.1520253419876099 train acc 0.6605225534009999\n",
            "epoch 2 batch id 6801 loss 1.4552098512649536 train acc 0.6608931590942508\n",
            "epoch 2 batch id 7001 loss 0.9988065958023071 train acc 0.6613073132409656\n",
            "epoch 2 batch id 7201 loss 1.3934506177902222 train acc 0.6614098736286627\n",
            "epoch 2 batch id 7401 loss 1.225451946258545 train acc 0.6617897919200109\n",
            "epoch 2 batch id 7601 loss 1.0572257041931152 train acc 0.6622648335745297\n",
            "epoch 2 batch id 7801 loss 1.1475303173065186 train acc 0.6625312459941033\n",
            "epoch 2 batch id 8001 loss 1.1367058753967285 train acc 0.6628605018122735\n",
            "epoch 2 batch id 8201 loss 1.7375885248184204 train acc 0.6632213297158883\n",
            "epoch 2 batch id 8401 loss 1.2337112426757812 train acc 0.6634980210689204\n",
            "epoch 2 batch id 8601 loss 1.4265527725219727 train acc 0.6637981775374956\n",
            "epoch 2 batch id 8801 loss 1.441781759262085 train acc 0.6641663589364845\n",
            "epoch 2 batch id 9001 loss 1.9774669408798218 train acc 0.6644956115987113\n",
            "epoch 2 batch id 9201 loss 1.3143274784088135 train acc 0.6647494158243669\n",
            "epoch 2 train acc 0.664982421387015\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dc11baa5b1842db9a3fa10ab108d3a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3113 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 test acc 0.6722423289103627\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c240944979e24621a12ce62d38991be2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3 batch id 1 loss 1.3999329805374146 train acc 0.640625\n",
            "epoch 3 batch id 201 loss 1.2283769845962524 train acc 0.6771610696517413\n",
            "epoch 3 batch id 401 loss 1.3979092836380005 train acc 0.6770963216957606\n",
            "epoch 3 batch id 601 loss 1.2656375169754028 train acc 0.679102537437604\n",
            "epoch 3 batch id 801 loss 1.3893213272094727 train acc 0.6791315543071161\n",
            "epoch 3 batch id 1001 loss 1.4830960035324097 train acc 0.678149975024975\n",
            "epoch 3 batch id 1201 loss 1.322819471359253 train acc 0.6779636761032473\n",
            "epoch 3 batch id 1401 loss 1.233636498451233 train acc 0.6786781763026409\n",
            "epoch 3 batch id 1601 loss 1.3890140056610107 train acc 0.6790872891942535\n",
            "epoch 3 batch id 1801 loss 1.5277866125106812 train acc 0.6791365907828983\n",
            "epoch 3 batch id 2001 loss 0.9341619610786438 train acc 0.6793322088955522\n",
            "epoch 3 batch id 2201 loss 1.025585412979126 train acc 0.6793431962744207\n",
            "epoch 3 batch id 2401 loss 1.010414958000183 train acc 0.6797428154935443\n",
            "epoch 3 batch id 2601 loss 1.1647870540618896 train acc 0.6801410515186467\n",
            "epoch 3 batch id 2801 loss 1.374537467956543 train acc 0.6805214655480185\n",
            "epoch 3 batch id 3001 loss 1.309470772743225 train acc 0.6812260496501167\n",
            "epoch 3 batch id 3201 loss 1.020810842514038 train acc 0.6819987894407997\n",
            "epoch 3 batch id 3401 loss 1.3639518022537231 train acc 0.6824279623640106\n",
            "epoch 3 batch id 3601 loss 1.3006330728530884 train acc 0.683074146070536\n",
            "epoch 3 batch id 3801 loss 1.245662808418274 train acc 0.6835536700868192\n",
            "epoch 3 batch id 4001 loss 0.936083197593689 train acc 0.6836454948762809\n",
            "epoch 3 batch id 4201 loss 1.6090960502624512 train acc 0.6840521601999524\n",
            "epoch 3 batch id 4401 loss 1.2569302320480347 train acc 0.6843473074301295\n",
            "epoch 3 batch id 4601 loss 1.283808946609497 train acc 0.684548875244512\n",
            "epoch 3 batch id 4801 loss 1.4034761190414429 train acc 0.6849940116642366\n",
            "epoch 3 batch id 5001 loss 1.3773874044418335 train acc 0.6855597630473905\n",
            "epoch 3 batch id 5201 loss 0.9106907248497009 train acc 0.6861511007498557\n",
            "epoch 3 batch id 5401 loss 1.4066920280456543 train acc 0.6864267033882614\n",
            "epoch 3 batch id 5601 loss 1.4063522815704346 train acc 0.6867523656489912\n",
            "epoch 3 batch id 5801 loss 1.483729600906372 train acc 0.6870771203240821\n",
            "epoch 3 batch id 6001 loss 1.0974634885787964 train acc 0.6876119605065822\n",
            "epoch 3 batch id 6201 loss 1.1713789701461792 train acc 0.6880392275439445\n",
            "epoch 3 batch id 6401 loss 1.066155195236206 train acc 0.6885179073582253\n",
            "epoch 3 batch id 6601 loss 1.051798939704895 train acc 0.6886693304044842\n",
            "epoch 3 batch id 6801 loss 1.3757126331329346 train acc 0.688986454197912\n",
            "epoch 3 batch id 7001 loss 0.8579949736595154 train acc 0.6894573096700471\n",
            "epoch 3 batch id 7201 loss 1.2606425285339355 train acc 0.6895374774336898\n",
            "epoch 3 batch id 7401 loss 1.0367168188095093 train acc 0.6899342149709499\n",
            "epoch 3 batch id 7601 loss 0.8821335434913635 train acc 0.6904066899092225\n",
            "epoch 3 batch id 7801 loss 1.0294424295425415 train acc 0.6905885463402128\n",
            "epoch 3 batch id 8001 loss 0.9782211184501648 train acc 0.6909331646044244\n",
            "epoch 3 batch id 8201 loss 1.637925386428833 train acc 0.6912743110596269\n",
            "epoch 3 batch id 8401 loss 1.2403528690338135 train acc 0.6915880549934531\n",
            "epoch 3 batch id 8601 loss 1.1758185625076294 train acc 0.6918563248459482\n",
            "epoch 3 batch id 8801 loss 1.2849159240722656 train acc 0.6921656629928418\n",
            "epoch 3 batch id 9001 loss 1.7147951126098633 train acc 0.6924508387956894\n",
            "epoch 3 batch id 9201 loss 1.1951874494552612 train acc 0.692586064014781\n",
            "epoch 3 train acc 0.6927607568619052\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f72f0a055ab14642836a5c9ef4d762e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3113 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3 test acc 0.6782048167002105\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deca083d7d9d42089f998c5b85cf2a37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4 batch id 1 loss 1.23025643825531 train acc 0.6875\n",
            "epoch 4 batch id 201 loss 1.1967414617538452 train acc 0.7008706467661692\n",
            "epoch 4 batch id 401 loss 1.3608778715133667 train acc 0.7030081047381546\n",
            "epoch 4 batch id 601 loss 1.1589761972427368 train acc 0.7045549084858569\n",
            "epoch 4 batch id 801 loss 1.2869644165039062 train acc 0.7055828651685393\n",
            "epoch 4 batch id 1001 loss 1.3487879037857056 train acc 0.7049044705294706\n",
            "epoch 4 batch id 1201 loss 1.0421518087387085 train acc 0.7045560990840966\n",
            "epoch 4 batch id 1401 loss 1.1482552289962769 train acc 0.7054893825838686\n",
            "epoch 4 batch id 1601 loss 1.1873624324798584 train acc 0.7058674266083698\n",
            "epoch 4 batch id 1801 loss 1.275618553161621 train acc 0.7061094530816213\n",
            "epoch 4 batch id 2001 loss 0.7807130813598633 train acc 0.7063421414292853\n",
            "epoch 4 batch id 2201 loss 0.8964746594429016 train acc 0.70629827351204\n",
            "epoch 4 batch id 2401 loss 0.9363118410110474 train acc 0.7069385152019991\n",
            "epoch 4 batch id 2601 loss 1.017949104309082 train acc 0.7075944348327566\n",
            "epoch 4 batch id 2801 loss 1.2360761165618896 train acc 0.7080004908961085\n",
            "epoch 4 batch id 3001 loss 1.1204278469085693 train acc 0.708586721092969\n",
            "epoch 4 batch id 3201 loss 0.8998576998710632 train acc 0.7093584036238676\n",
            "epoch 4 batch id 3401 loss 1.253036379814148 train acc 0.7096488165245516\n",
            "epoch 4 batch id 3601 loss 1.1237142086029053 train acc 0.7100805331852263\n",
            "epoch 4 batch id 3801 loss 1.1519367694854736 train acc 0.7105860299921073\n",
            "epoch 4 batch id 4001 loss 0.8282979130744934 train acc 0.7107988627843039\n",
            "epoch 4 batch id 4201 loss 1.3811787366867065 train acc 0.711288978814568\n",
            "epoch 4 batch id 4401 loss 1.163236379623413 train acc 0.7114895762326744\n",
            "epoch 4 batch id 4601 loss 1.212773084640503 train acc 0.7118730982395132\n",
            "epoch 4 batch id 4801 loss 1.3602931499481201 train acc 0.7122214122057905\n",
            "epoch 4 batch id 5001 loss 1.216477394104004 train acc 0.7127293291341732\n",
            "epoch 4 batch id 5201 loss 0.7904982566833496 train acc 0.7132612718707941\n",
            "epoch 4 batch id 5401 loss 1.3651660680770874 train acc 0.7135484169598223\n",
            "epoch 4 batch id 5601 loss 1.3053781986236572 train acc 0.7138206347080879\n",
            "epoch 4 batch id 5801 loss 1.4091081619262695 train acc 0.7141144845716256\n",
            "epoch 4 batch id 6001 loss 0.9608823657035828 train acc 0.7146621396433928\n",
            "epoch 4 batch id 6201 loss 1.0847666263580322 train acc 0.7149653281728754\n",
            "epoch 4 batch id 6401 loss 0.8997503519058228 train acc 0.7154936728636151\n",
            "epoch 4 batch id 6601 loss 0.88620924949646 train acc 0.7157272572337524\n",
            "epoch 4 batch id 6801 loss 1.3391025066375732 train acc 0.7160137295985884\n",
            "epoch 4 batch id 7001 loss 0.7277266979217529 train acc 0.7165315669190115\n",
            "epoch 4 batch id 7201 loss 1.2576920986175537 train acc 0.716604030690182\n",
            "epoch 4 batch id 7401 loss 0.9180585741996765 train acc 0.7170990406701797\n",
            "epoch 4 batch id 7601 loss 0.7189536690711975 train acc 0.717522776608341\n",
            "epoch 4 batch id 7801 loss 0.9693160057067871 train acc 0.7178226349186002\n",
            "epoch 4 batch id 8001 loss 0.8673694133758545 train acc 0.7180899262592176\n",
            "epoch 4 batch id 8201 loss 1.47196626663208 train acc 0.7183784751859529\n",
            "epoch 4 batch id 8401 loss 1.0267945528030396 train acc 0.7186384061421259\n",
            "epoch 4 batch id 8601 loss 1.0823978185653687 train acc 0.7188499157074759\n",
            "epoch 4 batch id 8801 loss 1.1541951894760132 train acc 0.7191015225542552\n",
            "epoch 4 batch id 9001 loss 1.5880494117736816 train acc 0.7193714587268081\n",
            "epoch 4 batch id 9201 loss 1.0241421461105347 train acc 0.7195141832409521\n",
            "epoch 4 train acc 0.7196272575579112\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a47d79d5a22a4315971bfa2099f33b26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3113 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4 test acc 0.679732212512374\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b934b0d57c194ee9a81819a50849ef66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5 batch id 1 loss 1.0855484008789062 train acc 0.6875\n",
            "epoch 5 batch id 201 loss 1.0484747886657715 train acc 0.7273009950248757\n",
            "epoch 5 batch id 401 loss 1.0955142974853516 train acc 0.7303615960099751\n",
            "epoch 5 batch id 601 loss 1.2289292812347412 train acc 0.7310732113144759\n",
            "epoch 5 batch id 801 loss 1.179101824760437 train acc 0.7308832709113608\n",
            "epoch 5 batch id 1001 loss 1.1172690391540527 train acc 0.7301448551448552\n",
            "epoch 5 batch id 1201 loss 0.8870865106582642 train acc 0.7292360532889259\n",
            "epoch 5 batch id 1401 loss 0.9987618327140808 train acc 0.729958511777302\n",
            "epoch 5 batch id 1601 loss 1.1202068328857422 train acc 0.7304711898813242\n",
            "epoch 5 batch id 1801 loss 1.1076183319091797 train acc 0.730097862298723\n",
            "epoch 5 batch id 2001 loss 0.6442956924438477 train acc 0.730181784107946\n",
            "epoch 5 batch id 2201 loss 0.818857729434967 train acc 0.7300232848705134\n",
            "epoch 5 batch id 2401 loss 0.8893573880195618 train acc 0.7304378384006663\n",
            "epoch 5 batch id 2601 loss 0.9617353677749634 train acc 0.7308006535947712\n",
            "epoch 5 batch id 2801 loss 1.156802773475647 train acc 0.7311451267404498\n",
            "epoch 5 batch id 3001 loss 1.0436387062072754 train acc 0.7315321976007997\n",
            "epoch 5 batch id 3201 loss 0.6346529722213745 train acc 0.732271165260856\n",
            "epoch 5 batch id 3401 loss 1.1629899740219116 train acc 0.732514334019406\n",
            "epoch 5 batch id 3601 loss 1.0638692378997803 train acc 0.7327955776173285\n",
            "epoch 5 batch id 3801 loss 1.0408917665481567 train acc 0.7330143383320179\n",
            "epoch 5 batch id 4001 loss 0.7006049752235413 train acc 0.7330159647588103\n",
            "epoch 5 batch id 4201 loss 1.23915696144104 train acc 0.7332034039514401\n",
            "epoch 5 batch id 4401 loss 1.0493731498718262 train acc 0.7333170018177687\n",
            "epoch 5 batch id 4601 loss 1.107777714729309 train acc 0.7336991958269942\n",
            "epoch 5 batch id 4801 loss 1.2037978172302246 train acc 0.7339779473026453\n",
            "epoch 5 batch id 5001 loss 1.2092463970184326 train acc 0.7341844131173765\n",
            "epoch 5 batch id 5201 loss 0.7092803716659546 train acc 0.7345943087867718\n",
            "epoch 5 batch id 5401 loss 1.3467638492584229 train acc 0.7347655526754304\n",
            "epoch 5 batch id 5601 loss 1.1828787326812744 train acc 0.7349440948044992\n",
            "epoch 5 batch id 5801 loss 1.3771494626998901 train acc 0.7350052792621962\n",
            "epoch 5 batch id 6001 loss 0.8264385461807251 train acc 0.735283702716214\n",
            "epoch 5 batch id 6201 loss 1.0655912160873413 train acc 0.7354484155781326\n",
            "epoch 5 batch id 6401 loss 0.8587592840194702 train acc 0.7358835533510389\n",
            "epoch 5 batch id 6601 loss 0.8185876607894897 train acc 0.7359561998182094\n",
            "epoch 5 batch id 6801 loss 1.2458858489990234 train acc 0.7361394464049404\n",
            "epoch 5 batch id 7001 loss 0.6216855645179749 train acc 0.7363390051421226\n",
            "epoch 5 batch id 7201 loss 1.0449743270874023 train acc 0.7362367205943618\n",
            "epoch 5 batch id 7401 loss 0.8738265037536621 train acc 0.7364714227807053\n",
            "epoch 5 batch id 7601 loss 0.6549898982048035 train acc 0.7367513320615708\n",
            "epoch 5 batch id 7801 loss 0.8990013003349304 train acc 0.7367725291629278\n",
            "epoch 5 batch id 8001 loss 0.8461114168167114 train acc 0.7369938132733408\n",
            "epoch 5 batch id 8201 loss 1.442905068397522 train acc 0.7371871570540178\n",
            "epoch 5 batch id 8401 loss 0.9591236114501953 train acc 0.7372931793834068\n",
            "epoch 5 batch id 8601 loss 0.9756986498832703 train acc 0.7373452214858738\n",
            "epoch 5 batch id 8801 loss 1.21457040309906 train acc 0.7373806953755255\n",
            "epoch 5 batch id 9001 loss 1.588194727897644 train acc 0.7374736140428841\n",
            "epoch 5 batch id 9201 loss 1.0134918689727783 train acc 0.7374079583740898\n",
            "epoch 5 train acc 0.7373921413784488\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a656f04a12b44899a2eb848c73dcf30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3113 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5 test acc 0.6805051807102539\n"
          ]
        }
      ],
      "source": [
        "# train - mid\n",
        "model2 = BERTClassifier2(bertmodel, dr_rate=0.5).to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model2.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model2.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "t_total = len(train_dataloader2) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    model2.train()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader2)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model2(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model2.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader2)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model2(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model2, 'model/model2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
